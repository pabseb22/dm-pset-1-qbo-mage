import json
import psycopg2
from psycopg2.extras import execute_values
from pandas import DataFrame

from mage_ai.data_preparation.shared.secrets import get_secret_value

if "data_exporter" not in globals():
    from mage_ai.data_preparation.decorators import data_exporter


def _require_secret(key: str) -> str:
    value = get_secret_value(key)
    if not value:
        raise ValueError(f"Missing required Mage Secret: {key}")
    return value


def _to_json(x):
    if x is None:
        return None
    if isinstance(x, (dict, list)):
        return json.dumps(x)
    return x


@data_exporter
def export_data_to_postgres(df: DataFrame, **kwargs) -> None:
    schema = "raw"
    table = "qb_customers"

    if df is None or df.empty:
        print("No rows to export.")
        return

    host = _require_secret("POSTGRES_HOST")
    port = int(_require_secret("POSTGRES_PORT"))
    db = _require_secret("POSTGRES_DB")
    user = _require_secret("POSTGRES_USER")
    password = _require_secret("POSTGRES_PASSWORD")

    # Deduplicate inside batch
    df2 = df.sort_values(
        ["extract_window_end_utc", "page_number"]
    ).drop_duplicates(subset=["id"], keep="last")

    ids = df2["id"].tolist()

    conn = psycopg2.connect(
        host=host,
        port=port,
        dbname=db,
        user=user,
        password=password,
    )
    conn.autocommit = False

    try:
        with conn.cursor() as cur:
            cur.execute(
                f"SELECT id FROM {schema}.{table} WHERE id = ANY(%s)",
                (ids,),
            )
            existing_ids = {row[0] for row in cur.fetchall()}

            inserted = 0
            updated = 0

            values = []
            for row in df2.to_dict(orient="records"):
                if row["id"] in existing_ids:
                    updated += 1
                else:
                    inserted += 1

                values.append((
                    row["id"],
                    _to_json(row["payload"]),
                    row["extract_window_start_utc"],
                    row["extract_window_end_utc"],
                    int(row["page_number"]),
                    int(row["page_size"]),
                    _to_json(row.get("request_payload")),
                ))

            upsert_sql = f"""
                INSERT INTO {schema}.{table}
                  (id, payload, extract_window_start_utc, extract_window_end_utc,
                   page_number, page_size, request_payload)
                VALUES %s
                ON CONFLICT (id) DO UPDATE SET
                  payload = EXCLUDED.payload::jsonb,
                  ingested_at_utc = now(),
                  extract_window_start_utc = EXCLUDED.extract_window_start_utc,
                  extract_window_end_utc = EXCLUDED.extract_window_end_utc,
                  page_number = EXCLUDED.page_number,
                  page_size = EXCLUDED.page_size,
                  request_payload = EXCLUDED.request_payload::jsonb;
            """

            execute_values(cur, upsert_sql, values, page_size=500)

        conn.commit()

        print(
            f"Export completed | inserted={inserted} "
            f"updated={updated} total={len(values)}"
        )

    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()