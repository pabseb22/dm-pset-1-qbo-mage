import io
import os
import time
import json
import base64
import pandas as pd
import requests

from datetime import datetime, timedelta, timezone
from dateutil.parser import isoparse
from mage_ai.data_preparation.shared.secrets import get_secret_value

if 'data_loader' not in globals():
    from mage_ai.data_preparation.decorators import data_loader
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test


def _require_secret(key: str) -> str:
    """
    Read a required secret from Mage Secrets.
    """
    val = get_secret_value(key)
    if not val:
        raise ValueError(f"Missing required secret: {key}")
    return val


def _refresh_access_token() -> str:
    """
    Exchange a refresh_token for a short-lived access_token (OAuth2).
    This is done automatically each pipeline run.
    """
    client_id = _require_secret("QBO_CLIENT_ID")
    client_secret = _require_secret("QBO_CLIENT_SECRET")
    refresh_token = _require_secret("QBO_REFRESH_TOKEN")

    token_url = "https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer"
    basic = base64.b64encode(f"{client_id}:{client_secret}".encode()).decode()

    headers = {
        "Authorization": f"Basic {basic}",
        "Accept": "application/json",
        "Content-Type": "application/x-www-form-urlencoded",
    }
    data = {
        "grant_type": "refresh_token",
        "refresh_token": refresh_token,
    }

    r = requests.post(token_url, headers=headers, data=data, timeout=30)
    r.raise_for_status()
    return r.json()["access_token"]


def _qbo_query(access_token: str, realm_id: str, query: str, minorversion: int = 65) -> dict:
    """
    Execute a QBO SQL-like query via the Query endpoint.
    Includes a small retry for transient errors / rate limits.
    """
    url = f"https://quickbooks.api.intuit.com/v3/company/{realm_id}/query?minorversion={minorversion}"
    headers = {
        "Authorization": f"Bearer {access_token}",
        "Accept": "application/json",
        "Content-Type": "application/text",
    }

    r = requests.post(url, headers=headers, data=query, timeout=60)

    # Small retry for common transient status codes
    if r.status_code in (429, 500, 502, 503, 504):
        time.sleep(2)
        r = requests.post(url, headers=headers, data=query, timeout=60)

    r.raise_for_status()
    return r.json()


def _to_utc(dt: datetime) -> datetime:
    """Force a datetime to UTC."""
    if dt.tzinfo is None:
        return dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(timezone.utc)


def _iter_daily_windows(start_utc: datetime, end_utc: datetime):
    """
    Split a large date range into daily windows to avoid very large queries.
    """
    current = start_utc
    while current < end_utc:
        nxt = min(current + timedelta(days=1), end_utc)
        yield current, nxt
        current = nxt


def _default_last_30_days_window():
    """
    Default extraction window: last 30 days up to 'now' in UTC.
    """
    end_utc = datetime.now(timezone.utc)
    start_utc = end_utc - timedelta(days=30)
    return start_utc, end_utc


@data_loader
def load_data_from_api(*args, **kwargs):
    """
    Customers backfill loader.

    Runtime params (optional):
      - start_utc: ISO-8601 (e.g. "2025-01-01T00:00:00Z")
      - end_utc:   ISO-8601 (e.g. "2025-01-05T00:00:00Z")
      - page_size: int (default 200)

    Default behavior:
      If start_utc/end_utc are not provided, the loader fetches data from the last 30 days.

    Output:
      A pandas DataFrame with one row per Customer record (may be empty if no updates exist).
    """
    realm_id = _require_secret("QBO_REALM_ID")

    start_str = kwargs.get("start_utc")
    end_str = kwargs.get("end_utc")

    # If user didn't pass a window, default to last 30 days
    if not start_str or not end_str:
        start_utc, end_utc = _default_last_30_days_window()
    else:
        start_utc = _to_utc(isoparse(start_str))
        end_utc = _to_utc(isoparse(end_str))

    page_size = int(kwargs.get("page_size", 200))

    access_token = _refresh_access_token()
    rows = []

    for w_start, w_end in _iter_daily_windows(start_utc, end_utc):
        w_start_s = w_start.strftime("%Y-%m-%dT%H:%M:%SZ")
        w_end_s = w_end.strftime("%Y-%m-%dT%H:%M:%SZ")

        start_pos = 1
        page_number = 1

        while True:
            query = (
                "SELECT * FROM Customer "
                f"WHERE Metadata.LastUpdatedTime >= '{w_start_s}' "
                f"AND Metadata.LastUpdatedTime < '{w_end_s}' "
                f"STARTPOSITION {start_pos} MAXRESULTS {page_size}"
            )

            payload = _qbo_query(access_token, realm_id, query)
            customers = (payload.get("QueryResponse", {}) or {}).get("Customer", []) or []

            for c in customers:
                cid = str(c.get("Id"))
                rows.append({
                    "id": cid,
                    "payload": json.dumps(c),
                    "extract_window_start_utc": w_start_s,
                    "extract_window_end_utc": w_end_s,
                    "page_number": page_number,
                    "page_size": page_size,
                    "request_payload": json.dumps({"query": query}),
                })

            if len(customers) < page_size:
                break

            start_pos += page_size
            page_number += 1

    # Returning an empty DataFrame is OK (it means no records were updated in that window)
    return pd.DataFrame(rows)


@test
def test_output(output, *args) -> None:
    """
    Basic validation:
    - output must exist
    - if there are rows, required columns must be present
    """
    assert output is not None, 'The output is undefined'

    if hasattr(output, "empty") and not output.empty:
        required_cols = {
            "id",
            "payload",
            "extract_window_start_utc",
            "extract_window_end_utc",
            "page_number",
            "page_size",
            "request_payload",
        }
        assert required_cols.issubset(set(output.columns)), f"Missing columns: {required_cols - set(output.columns)}"