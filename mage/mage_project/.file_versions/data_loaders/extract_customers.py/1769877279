import io
import os
import time
import json
import base64
import pandas as pd
import requests

from datetime import datetime, timedelta, timezone
from dateutil.parser import isoparse
from mage_ai.data_preparation.shared.secrets import get_secret_value

if 'data_loader' not in globals():
    from mage_ai.data_preparation.decorators import data_loader
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test


def _require_env(key: str) -> str:
    """
    Read a required secret from environment variables.
    In Mage, Secrets are commonly injected into the runtime as env vars.
    """
    val = os(key)
    if not val:
        raise ValueError(f"Missing required secret: {key}")
    return val


def _refresh_access_token() -> str:
    """
    Exchange a refresh_token for a short-lived access_token (OAuth2).
    This should be done automatically every pipeline run.
    """
    client_id = _require_env("QBO_CLIENT_ID")
    client_secret = _require_env("QBO_CLIENT_SECRET")
    refresh_token = _require_env("QBO_REFRESH_TOKEN")

    token_url = "https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer"
    basic = base64.b64encode(f"{client_id}:{client_secret}".encode()).decode()

    headers = {
        "Authorization": f"Basic {basic}",
        "Accept": "application/json",
        "Content-Type": "application/x-www-form-urlencoded",
    }
    data = {
        "grant_type": "refresh_token",
        "refresh_token": refresh_token,
    }

    r = requests.post(token_url, headers=headers, data=data, timeout=30)
    r.raise_for_status()
    return r.json()["access_token"]


def _qbo_query(access_token: str, realm_id: str, query: str, minorversion: int = 65) -> dict:
    """
    Execute a QBO SQL-like query via the Query endpoint.
    Includes a small retry for transient errors / rate limits.
    """
    url = f"https://quickbooks.api.intuit.com/v3/company/{realm_id}/query?minorversion={minorversion}"
    headers = {
        "Authorization": f"Bearer {access_token}",
        "Accept": "application/json",
        "Content-Type": "application/text",
    }

    r = requests.post(url, headers=headers, data=query, timeout=60)

    # Simple retry for common transient status codes (rate limiting / gateway)
    if r.status_code in (429, 500, 502, 503, 504):
        time.sleep(2)
        r = requests.post(url, headers=headers, data=query, timeout=60)

    r.raise_for_status()
    return r.json()


def _to_utc(dt: datetime) -> datetime:
    """Force a datetime to UTC."""
    if dt.tzinfo is None:
        return dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(timezone.utc)


def _iter_daily_windows(start_utc: datetime, end_utc: datetime):
    """
    Split a large date range into daily windows to avoid very large queries.
    This also makes backfills more restart-friendly.
    """
    current = start_utc
    while current < end_utc:
        nxt = min(current + timedelta(days=1), end_utc)
        yield current, nxt
        current = nxt


@data_loader
def load_data_from_api(*args, **kwargs):
    """
    Customers backfill loader.

    Expected run params in Mage:
      - start_utc: ISO-8601 string (e.g. "2025-01-01T00:00:00Z")
      - end_utc:   ISO-8601 string (e.g. "2025-01-05T00:00:00Z")
      - page_size: optional int (default 200)

    Output:
      A pandas DataFrame with one row per Customer record, prepared for upsert into Postgres RAW.
    """
    realm_id = _require_env("QBO_REALM_ID")
    start_str = kwargs.get("start_utc")
    end_str = kwargs.get("end_utc")
    if not start_str or not end_str:
        raise ValueError("Missing params: start_utc and end_utc (ISO-8601)")

    start_utc = _to_utc(isoparse(start_str))
    end_utc = _to_utc(isoparse(end_str))
    page_size = int(kwargs.get("page_size", 200))

    access_token = _refresh_access_token()

    rows = []

    # Query by last updated time inside each daily window
    for w_start, w_end in _iter_daily_windows(start_utc, end_utc):
        w_start_s = w_start.strftime("%Y-%m-%dT%H:%M:%SZ")
        w_end_s = w_end.strftime("%Y-%m-%dT%H:%M:%SZ")

        start_pos = 1
        page_number = 1

        while True:
            # QBO supports STARTPOSITION and MAXRESULTS pagination in queries
            query = (
                "SELECT * FROM Customer "
                f"WHERE Metadata.LastUpdatedTime >= '{w_start_s}' "
                f"AND Metadata.LastUpdatedTime < '{w_end_s}' "
                f"STARTPOSITION {start_pos} MAXRESULTS {page_size}"
            )

            payload = _qbo_query(access_token, realm_id, query)
            customers = (payload.get("QueryResponse", {}) or {}).get("Customer", []) or []

            for c in customers:
                cid = str(c.get("Id"))
                rows.append({
                    # Primary key for idempotent upserts
                    "id": cid,

                    # Store the full record as JSON string (will be cast to JSONB on insert)
                    "payload": json.dumps(c),

                    # Ingestion metadata (window + pagination)
                    "extract_window_start_utc": w_start_s,
                    "extract_window_end_utc": w_end_s,
                    "page_number": page_number,
                    "page_size": page_size,

                    # Keep the exact query used for traceability/debugging
                    "request_payload": json.dumps({"query": query}),
                })

            # If fewer results than page_size, we reached the end of this window
            if len(customers) < page_size:
                break

            start_pos += page_size
            page_number += 1

    return pd.DataFrame(rows)


@test
def test_output(output, *args) -> None:
    """
    Basic validation:
    - output must exist
    - if there are rows, required columns must be present
    """
    assert output is not None, 'The output is undefined'

    if hasattr(output, "empty") and not output.empty:
        required_cols = {
            "id",
            "payload",
            "extract_window_start_utc",
            "extract_window_end_utc",
            "page_number",
            "page_size",
            "request_payload",
        }
        assert required_cols.issubset(set(output.columns)), f"Missing columns: {required_cols - set(output.columns)}"
